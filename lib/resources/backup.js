// Copyright 2020 ODK Central Developers
// See the NOTICE file at the top-level directory of this distribution and at
// https://github.com/opendatakit/central-backend/blob/master/NOTICE.
// This file is part of ODK Central. It is subject to the license terms in
// the LICENSE file found in the top-level directory of this distribution and at
// https://www.apache.org/licenses/LICENSE-2.0. No part of ODK Central,
// including this file, may be copied, modified, propagated, or distributed
// except according to the terms contained in the LICENSE file.

const { createReadStream, readdir, unlinkSync } = require('fs');
const { promisify } = require('util');
const { basename, join } = require('path');
const { merge } = require('ramda');
const archiver = require('archiver');
const { tmpdir } = require('../task/fs');
const { pgdump } = require('../task/db');
const { generateManagedKey, generateLocalCipherer } = require('../util/crypto');
const { getOrNotFound } = require('../util/promise');


const backup = (keys, response) => {
  const zipStream = archiver('zip', { zlib: { level: 9 } });

  // manually write out our headers for the keepalive mechanism.
  // TODO: obviously directly accessing internal methods is bad..
  response._writeRaw('HTTP/1.1 200 OK\r\n');
  response._writeRaw('Connection: keep-alive\r\n');
  response._writeRaw(`Content-Disposition: attachment; filename="central-backup-${(new Date()).toISOString()}.zip"\r\n`);
  response._writeRaw('Content-Type: application/zip\r\n');
  response._writeRaw(`Date: ${(new Date()).toUTCString()}\r\n`);
  response._writeRaw('Transfer-Encoding: chunked\r\n');

  // keep our connection alive by writing a header every 5s.
  let counter = 0;
  const keepalive = setInterval(() => {
    counter += 1;
    response._writeRaw(`X-Keepalive: ${counter}\r\n`);
  }, 5000);

  // get our configuration and obtain a tmpdir to dump the database into.
  tmpdir().then(([ tmpdirPath, tmpdirRm ]) => {
    zipStream.on('end', () => { tmpdirRm(); });

    pgdump(tmpdirPath).then(() => {
      // halt the keepalive.
      clearInterval(keepalive);
      response._headerSent = true;
      response.flushHeaders();

      // TODO: mostly copypasta for now from lib/task/fs:
      promisify(readdir)(tmpdirPath).then((files) => {
        // create a cipher-generator for use below.
        const [ localkey, cipherer ] = generateLocalCipherer(keys);
        const local = { key: localkey, ivs: {} };

        // add each file generated by the backup process.
        for (const file of files) {
          const filePath = join(tmpdirPath, file);
          const [ iv, cipher ] = cipherer();
          local.ivs[basename(file)] = iv.toString('base64');

          const readStream = createReadStream(filePath);
          zipStream.append(readStream.pipe(cipher), { name: file });
          readStream.on('end', () => unlinkSync(filePath)); // sync to ensure completion.
        }
        zipStream.append(JSON.stringify(merge(keys, { local })), { name: 'keys.json' });
        zipStream.finalize();
      });
    });
  });

  return zipStream;
};


module.exports = (service, endpoint) => {
  service.get('/backup', endpoint(({ Config }, { auth }, _, response) =>
    auth.canOrReject('backup.run', Config.species())
      .then(() => Config.get('backups.main'))
      .then(getOrNotFound)
      .then((config) => backup(JSON.parse(config.value).keys, response))));

  service.post('/backup', endpoint(({ Config }, { auth, body }, _, response) =>
    auth.canOrReject('backup.run', Config.species())
      .then(() => generateManagedKey(body.passphrase))
      .then((keys) => backup(keys, response))));
};

